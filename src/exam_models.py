"""
Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ PyTorch Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, List, Dict, Any, Optional


class ExamMLP(nn.Module):
    """
    Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ù¾ÛŒØ´â€ŒØ®ÙˆØ± Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ±
    """
    
    def __init__(self, input_dim: int, hidden_dims: Tuple[int, ...] = (128, 64), 
                 num_classes: int = 2, dropout_rate: float = 0.2,
                 activation: str = 'relu', batch_norm: bool = True):
        """
        Parameters:
        -----------
        input_dim : int
            Ø¨Ø¹Ø¯ ÙˆØ±ÙˆØ¯ÛŒ
        hidden_dims : Tuple[int, ...]
            Ø§Ø¨Ø¹Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù†
        num_classes : int
            ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
        dropout_rate : float
            Ù†Ø±Ø® Dropout
        activation : str
            ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²
        batch_norm : bool
            Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Batch Normalization
        """
        super(ExamMLP, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.num_classes = num_classes
        self.dropout_rate = dropout_rate
        self.activation = activation
        self.use_batch_norm = batch_norm
        
        # Ø³Ø§Ø®Øª Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§
        layers = []
        prev_dim = input_dim
        
        for i, hidden_dim in enumerate(hidden_dims):
            # Ù„Ø§ÛŒÙ‡ Ø®Ø·ÛŒ
            layers.append(nn.Linear(prev_dim, hidden_dim))
            
            # Batch Normalization
            if batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))
            
            # ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²
            if activation == 'relu':
                layers.append(nn.ReLU())
            elif activation == 'leaky_relu':
                layers.append(nn.LeakyReLU(0.1))
            elif activation == 'selu':
                layers.append(nn.SELU())
            elif activation == 'tanh':
                layers.append(nn.Tanh())
            
            # Dropout
            if dropout_rate > 0:
                layers.append(nn.Dropout(dropout_rate))
            
            prev_dim = hidden_dim
        
        # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ
        if num_classes > 1:
            layers.append(nn.Linear(prev_dim, num_classes))
        else:
            layers.append(nn.Linear(prev_dim, 1))  # Ø¨Ø±Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†
        
        self.model = nn.Sequential(*layers)
        
        # ØªÙ†Ø¸ÛŒÙ… ÙˆØ²Ù†â€ŒÙ‡Ø§
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ²Ù†â€ŒÙ‡Ø§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Ù¾ÛŒØ´â€ŒØ¨Ø±Ø¯ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ø´Ø¨Ú©Ù‡"""
        return self.model(x)
    
    def get_feature_importance(self, X: np.ndarray, feature_names: List[str]) -> Dict[str, float]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„"""
        if len(feature_names) != self.input_dim:
            raise ValueError(f"ØªØ¹Ø¯Ø§Ø¯ Ù†Ø§Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ({len(feature_names)}) Ø¨Ø§ Ø¨Ø¹Ø¯ ÙˆØ±ÙˆØ¯ÛŒ ({self.input_dim}) Ù…Ø·Ø§Ø¨Ù‚Øª Ù†Ø¯Ø§Ø±Ø¯")
        
        # ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„
        first_layer = self.model[0]
        weights = first_layer.weight.data.cpu().numpy()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‚Ø¯Ø± Ù…Ø·Ù„Ù‚ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ)
        importance = np.mean(np.abs(weights), axis=0)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ
        feature_importance = dict(zip(feature_names, importance))
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù‡Ù…ÛŒØª
        feature_importance = dict(sorted(feature_importance.items(), 
                                        key=lambda x: x[1], reverse=True))
        
        return feature_importance


class ExamTabTransformer(nn.Module):
    """
    Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ TabTransformer Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ±
    """
    
    def __init__(self, num_categorical: int, num_continuous: int, categories: List[int],
                 num_classes: int = 2, embedding_dim: int = 32, num_heads: int = 4,
                 num_layers: int = 4, transformer_dropout: float = 0.1,
                 mlp_hidden: Tuple[int, ...] = (128, 64), mlp_dropout: float = 0.2):
        """
        Parameters:
        -----------
        num_categorical : int
            ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        num_continuous : int
            ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        categories : List[int]
            ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        num_classes : int
            ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
        embedding_dim : int
            Ø¨Ø¹Ø¯ embedding
        num_heads : int
            ØªØ¹Ø¯Ø§Ø¯ headÙ‡Ø§ÛŒ attention
        num_layers : int
            ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Transformer
        transformer_dropout : float
            Ù†Ø±Ø® Dropout Ø¯Ø± Transformer
        mlp_hidden : Tuple[int, ...]
            Ø§Ø¨Ø¹Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† MLP
        mlp_dropout : float
            Ù†Ø±Ø® Dropout Ø¯Ø± MLP
        """
        super(ExamTabTransformer, self).__init__()
        
        self.num_categorical = num_categorical
        self.num_continuous = num_continuous
        self.categories = categories
        self.num_classes = num_classes
        self.embedding_dim = embedding_dim
        
        # Embedding Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        self.embeddings = nn.ModuleList([
            nn.Embedding(num_categories, embedding_dim) 
            for num_categories in categories
        ])
        
        # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=num_heads,
            dim_feedforward=embedding_dim * 4,
            dropout=transformer_dropout,
            activation='gelu',
            batch_first=True
        )
        
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )
        
        # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ MLP Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if num_continuous > 0:
            self.continuous_projection = nn.Sequential(
                nn.Linear(num_continuous, embedding_dim),
                nn.LayerNorm(embedding_dim),
                nn.ReLU(),
                nn.Dropout(transformer_dropout)
            )
        
        # MLP Ù†Ù‡Ø§ÛŒÛŒ
        mlp_input_dim = embedding_dim * num_categorical + (embedding_dim if num_continuous > 0 else 0)
        
        mlp_layers = []
        prev_dim = mlp_input_dim
        
        for hidden_dim in mlp_hidden:
            mlp_layers.append(nn.Linear(prev_dim, hidden_dim))
            mlp_layers.append(nn.LayerNorm(hidden_dim))
            mlp_layers.append(nn.ReLU())
            mlp_layers.append(nn.Dropout(mlp_dropout))
            prev_dim = hidden_dim
        
        if num_classes > 1:
            mlp_layers.append(nn.Linear(prev_dim, num_classes))
        else:
            mlp_layers.append(nn.Linear(prev_dim, 1))
        
        self.mlp = nn.Sequential(*mlp_layers)
        
        # ØªÙ†Ø¸ÛŒÙ… ÙˆØ²Ù†â€ŒÙ‡Ø§
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ²Ù†â€ŒÙ‡Ø§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.01)
            elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, categorical: torch.Tensor, continuous: torch.Tensor) -> torch.Tensor:
        """
        Ù¾ÛŒØ´â€ŒØ¨Ø±Ø¯ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„
        
        Parameters:
        -----------
        categorical : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_categorical)
        continuous : torch.Tensor
            ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (batch_size, num_continuous)
        """
        # Embedding ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        embedded_categorical = []
        for i in range(self.num_categorical):
            emb = self.embeddings[i](categorical[:, i])
            embedded_categorical.append(emb)
        
        # ØªØ±Ú©ÛŒØ¨ embeddingâ€ŒÙ‡Ø§
        embedded_categorical = torch.stack(embedded_categorical, dim=1)  # (batch, num_cat, emb_dim)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Transformer
        transformer_output = self.transformer(embedded_categorical)
        
        # Flatten Ú©Ø±Ø¯Ù† Ø®Ø±ÙˆØ¬ÛŒ Transformer
        transformer_flattened = transformer_output.reshape(
            transformer_output.size(0), -1
        )  # (batch, num_cat * emb_dim)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        if self.num_continuous > 0:
            continuous_projected = self.continuous_projection(continuous)
            combined = torch.cat([transformer_flattened, continuous_projected], dim=1)
        else:
            combined = transformer_flattened
        
        # MLP Ù†Ù‡Ø§ÛŒÛŒ
        output = self.mlp(combined)
        
        return output
    
    def get_attention_weights(self, categorical: torch.Tensor) -> torch.Tensor:
        """
        Ø¯Ø±ÛŒØ§ÙØª ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ attention Ø¨Ø±Ø§ÛŒ ØªÙØ³ÛŒØ±Ù¾Ø°ÛŒØ±ÛŒ
        """
        self.eval()
        with torch.no_grad():
            # Embedding ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
            embedded_categorical = []
            for i in range(self.num_categorical):
                emb = self.embeddings[i](categorical[:, i])
                embedded_categorical.append(emb)
            
            embedded_categorical = torch.stack(embedded_categorical, dim=1)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ attention
            attention_weights = []
            for layer in self.transformer.layers:
                # self-attention Ø¯Ø± Ù‡Ø± Ù„Ø§ÛŒÙ‡
                _, attn_weights = layer.self_attn(
                    embedded_categorical, embedded_categorical, embedded_categorical,
                    need_weights=True, average_attn_weights=True
                )
                attention_weights.append(attn_weights)
            
            # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§
            avg_attention = torch.mean(torch.stack(attention_weights), dim=0)
            
            return avg_attention


class ExamRegressor(nn.Module):
    """
    Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ØªØ¨Ù‡ Ú©Ù†Ú©ÙˆØ±
    """
    
    def __init__(self, input_dim: int, hidden_dims: Tuple[int, ...] = (128, 64), 
                 dropout_rate: float = 0.2, activation: str = 'relu'):
        super(ExamRegressor, self).__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            
            if activation == 'relu':
                layers.append(nn.ReLU())
            elif activation == 'leaky_relu':
                layers.append(nn.LeakyReLU(0.1))
            elif activation == 'selu':
                layers.append(nn.SELU())
            
            if dropout_rate > 0:
                layers.append(nn.Dropout(dropout_rate))
            
            prev_dim = hidden_dim
        
        # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ (ÛŒÚ© Ù†ÙˆØ±ÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†)
        layers.append(nn.Linear(prev_dim, 1))
        
        self.model = nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Ù¾ÛŒØ´â€ŒØ¨Ø±Ø¯ Ø¯Ø§Ø¯Ù‡"""
        return self.model(x).squeeze()  # Ø­Ø°Ù Ø¨Ø¹Ø¯ Ø§Ø¶Ø§ÙÛŒ


class EarlyStopping:
    """
    Early Stopping Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting
    """
    
    def __init__(self, patience: int = 10, min_delta: float = 0.001, 
                 verbose: bool = True, restore_best_weights: bool = True):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.restore_best_weights = restore_best_weights
        
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_model_state = None
    
    def __call__(self, score: float, model: nn.Module) -> None:
        """
        Ø¨Ø±Ø±Ø³ÛŒ Early Stopping
        
        Parameters:
        -----------
        score : float
            Ø§Ù…ØªÛŒØ§Ø² validation (Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ØªØ± Ø§Ø³Øª)
        model : nn.Module
            Ù…Ø¯Ù„ PyTorch
        """
        if self.best_score is None:
            self.best_score = score
            self.best_model_state = model.state_dict().copy()
            if self.verbose:
                print(f"    ğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ø§ÙˆÙ„ÛŒÙ‡: {self.best_score:.4f}")
        
        elif score - self.best_score > self.min_delta:
            self.best_score = score
            self.best_model_state = model.state_dict().copy()
            self.counter = 0
            if self.verbose:
                print(f"    ğŸ“ˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ù‡: {self.best_score:.4f}")
        
        else:
            self.counter += 1
            if self.verbose:
                print(f"    â³ Ø¹Ø¯Ù… Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø±Ø§ÛŒ {self.counter}/{self.patience} epoch")
            
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print("    ğŸ›‘ ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… ÙØ¹Ø§Ù„ Ø´Ø¯")
                
                if self.restore_best_weights and self.best_model_state:
                    model.load_state_dict(self.best_model_state)
                    if self.verbose:
                        print("    ğŸ”„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† ÙˆØ²Ù†â€ŒÙ‡Ø§")
    
    def reset(self):
        """Ø¨Ø§Ø²Ù†Ø´Ø§Ù†ÛŒ Early Stopping"""
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_model_state = None