"""
ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
"""

import os
import pickle
import json
import numpy as np
import pandas as pd
import torch
import shutil
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')


class ExamModelUtils:
    """
    Ú©Ù„Ø§Ø³ ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ±
    """
    
    @staticmethod
    def create_model_name(model_type: str, config: Dict[str, Any], 
                         fold: Optional[int] = None, version: str = 'v1') -> str:
        """
        Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø§Ù… Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„
        
        Parameters:
        -----------
        model_type : str
            Ù†ÙˆØ¹ Ù…Ø¯Ù„
        config : Dict[str, Any]
            Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„
        fold : int, optional
            Ø´Ù…Ø§Ø±Ù‡ fold
        version : str
            Ù†Ø³Ø®Ù‡ Ù…Ø¯Ù„
        
        Returns:
        --------
        str
            Ù†Ø§Ù… Ù…Ø¯Ù„
        """
        # Ù†Ø§Ù… Ù¾Ø§ÛŒÙ‡
        if fold is not None:
            base_name = f"{model_type}_fold{fold}_{version}"
        else:
            base_name = f"{model_type}_{version}"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ù‡Ù…
        param_parts = []
        
        if model_type == 'MLP':
            important_params = ['hidden_layers', 'dropout_rate', 'learning_rate', 'batch_size']
        elif model_type == 'TabTransformer':
            important_params = ['embedding_dim', 'num_heads', 'num_layers', 'transformer_dropout']
        elif model_type == 'RandomForest':
            important_params = ['n_estimators', 'max_depth', 'min_samples_split']
        elif model_type == 'XGBoost':
            important_params = ['n_estimators', 'max_depth', 'learning_rate']
        elif model_type == 'LightGBM':
            important_params = ['n_estimators', 'num_leaves', 'learning_rate']
        elif model_type == 'SVM':
            important_params = ['C', 'kernel', 'gamma']
        elif model_type == 'Logistic':
            important_params = ['C', 'penalty', 'solver']
        else:
            important_params = []
        
        for param in important_params:
            if param in config:
                value = config[param]
                if isinstance(value, (list, tuple)):
                    value = '_'.join(str(v) for v in value)
                param_parts.append(f"{param[:3]}_{str(value).replace('.', '')}")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† timestamp
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        
        # ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§
        if param_parts:
            model_name = f"{base_name}_{'_'.join(param_parts)}_{timestamp}"
        else:
            model_name = f"{base_name}_{timestamp}"
        
        return model_name
    
    @staticmethod
    def save_model(model: Any, model_path: str, model_type: str, 
                  config: Dict[str, Any], scaler: Optional[Any] = None,
                  label_encoder: Optional[Any] = None, feature_names: Optional[List[str]] = None,
                  metrics: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·
        
        Parameters:
        -----------
        model : Any
            Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡
        model_path : str
            Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
        model_type : str
            Ù†ÙˆØ¹ Ù…Ø¯Ù„
        config : Dict[str, Any]
            Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„
        scaler : Any, optional
            Ø´ÛŒØ¡ scaler
        label_encoder : Any, optional
            Ø´ÛŒØ¡ label encoder
        feature_names : List[str], optional
            Ù†Ø§Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        metrics : Dict[str, float], optional
            Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        
        Returns:
        --------
        Dict[str, Any]
            Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
        """
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        save_info = {
            'model_type': model_type,
            'config': config,
            'save_time': pd.Timestamp.now().isoformat(),
            'feature_names': feature_names,
            'metrics': metrics
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ù…Ø¯Ù„
        if model_type in ['MLP', 'TabTransformer', 'Regressor']:
            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ PyTorch
            torch.save({
                'model_state_dict': model.state_dict(),
                'config': config,
                'model_type': model_type,
                'save_info': save_info
            }, model_path)
            
            # Ø°Ø®ÛŒØ±Ù‡ save_info Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
            info_path = model_path.replace('.pt', '_info.json')
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(save_info, f, ensure_ascii=False, indent=2)
        
        else:
            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ scikit-learn
            import joblib
            joblib.dump(model, model_path)
            
            # Ø°Ø®ÛŒØ±Ù‡ save_info
            info_path = model_path.replace('.pkl', '_info.json').replace('.joblib', '_info.json')
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(save_info, f, ensure_ascii=False, indent=2)
        
        # Ø°Ø®ÛŒØ±Ù‡ scaler Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
        if scaler is not None:
            scaler_path = model_path.replace('.pt', '_scaler.pkl').replace('.pkl', '_scaler.pkl').replace('.joblib', '_scaler.pkl')
            with open(scaler_path, 'wb') as f:
                pickle.dump(scaler, f)
            save_info['scaler_path'] = scaler_path
        
        # Ø°Ø®ÛŒØ±Ù‡ label encoder Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
        if label_encoder is not None:
            le_path = model_path.replace('.pt', '_label_encoder.pkl').replace('.pkl', '_label_encoder.pkl').replace('.joblib', '_label_encoder.pkl')
            with open(le_path, 'wb') as f:
                pickle.dump(label_encoder, f)
            save_info['label_encoder_path'] = le_path
        
        print(f"âœ… Ù…Ø¯Ù„ Ø¯Ø± {model_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        return save_info
    
    @staticmethod
    def load_model(model_path: str) -> Tuple[Any, Dict[str, Any]]:
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·
        
        Parameters:
        -----------
        model_path : str
            Ù…Ø³ÛŒØ± Ù…Ø¯Ù„
        
        Returns:
        --------
        Tuple[Any, Dict[str, Any]]
            Ù…Ø¯Ù„ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·
        """
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ ÛŒØ§ÙØª Ù†Ø´Ø¯: {model_path}")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„
        info_path = model_path.replace('.pt', '_info.json').replace('.pkl', '_info.json').replace('.joblib', '_info.json')
        
        if os.path.exists(info_path):
            with open(info_path, 'r', encoding='utf-8') as f:
                save_info = json.load(f)
        else:
            save_info = {}
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø³ÙˆÙ†Ø¯
        if model_path.endswith('.pt'):
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ PyTorch
            checkpoint = torch.load(model_path, map_location=torch.device('cpu'))
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ type
            model_type = checkpoint.get('model_type', 'MLP')
            config = checkpoint.get('config', {})
            
            if model_type == 'MLP':
                from exam_models import ExamMLP
                input_dim = config.get('input_dim', 10)
                hidden_dims = config.get('hidden_dims', (128, 64))
                num_classes = config.get('num_classes', 2)
                model = ExamMLP(input_dim, hidden_dims, num_classes)
            elif model_type == 'TabTransformer':
                from exam_models import ExamTabTransformer
                num_categorical = config.get('num_categorical', 3)
                num_continuous = config.get('num_continuous', 7)
                categories = config.get('categories', [10, 5, 8])
                num_classes = config.get('num_classes', 2)
                model = ExamTabTransformer(num_categorical, num_continuous, categories, num_classes)
            elif model_type == 'Regressor':
                from exam_models import ExamRegressor
                input_dim = config.get('input_dim', 10)
                hidden_dims = config.get('hidden_dims', (128, 64))
                model = ExamRegressor(input_dim, hidden_dims)
            else:
                raise ValueError(f"Ù†ÙˆØ¹ Ù…Ø¯Ù„ Ù†Ø§Ù…Ø¹ØªØ¨Ø±: {model_type}")
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
        
        else:
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ scikit-learn
            import joblib
            model = joblib.load(model_path)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ scaler Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
        scaler = None
        scaler_path = model_path.replace('.pt', '_scaler.pkl').replace('.pkl', '_scaler.pkl').replace('.joblib', '_scaler.pkl')
        if os.path.exists(scaler_path):
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            save_info['scaler'] = scaler
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ label encoder Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
        le_path = model_path.replace('.pt', '_label_encoder.pkl').replace('.pkl', '_label_encoder.pkl').replace('.joblib', '_label_encoder.pkl')
        if os.path.exists(le_path):
            with open(le_path, 'rb') as f:
                label_encoder = pickle.load(f)
            save_info['label_encoder'] = label_encoder
        
        print(f"âœ… Ù…Ø¯Ù„ Ø§Ø² {model_path} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        
        return model, save_info
    
    @staticmethod
    def remove_old_models(models_dir: str, model_type: str, keep_best: int = 3, 
                         metric: str = 'roc_auc_ovr') -> List[str]:
        """
        Ø­Ø°Ù Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ùˆ Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø±ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
        
        Parameters:
        -----------
        models_dir : str
            Ù¾ÙˆØ´Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        model_type : str
            Ù†ÙˆØ¹ Ù…Ø¯Ù„
        keep_best : int
            ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø±ÛŒ
        metric : str
            Ù…Ø¹ÛŒØ§Ø± Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
        
        Returns:
        --------
        List[str]
            Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡
        """
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ù†ÙˆØ¹ Ù…Ø´Ø®Øµ
        model_files = []
        for file in os.listdir(models_dir):
            if file.startswith(model_type) and (file.endswith('.pt') or file.endswith('.pkl') or file.endswith('.joblib')):
                model_files.append(file)
        
        if len(model_files) <= keep_best:
            print(f"âš ï¸ ØªÙ†Ù‡Ø§ {len(model_files)} Ù…Ø¯Ù„ {model_type} ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ø­Ø°Ù Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯")
            return []
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„
        model_metrics = []
        for model_file in model_files:
            model_path = os.path.join(models_dir, model_file)
            info_path = model_path.replace('.pt', '_info.json').replace('.pkl', '_info.json').replace('.joblib', '_info.json')
            
            score = 0
            save_time = ''
            
            if os.path.exists(info_path):
                with open(info_path, 'r', encoding='utf-8') as f:
                    info = json.load(f)
                
                metrics = info.get('metrics', {})
                score = metrics.get(metric, 0)
                save_time = info.get('save_time', '')
            
            model_metrics.append((model_file, score, save_time))
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø± (Ù†Ø²ÙˆÙ„ÛŒ)
        model_metrics.sort(key=lambda x: x[1], reverse=True)
        
        # Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø±ÛŒ Ùˆ Ø­Ø°Ù
        keep_models = [m[0] for m in model_metrics[:keep_best]]
        remove_models = [m[0] for m in model_metrics[keep_best:]]
        
        # Ø­Ø°Ù Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
        removed = []
        for model_file in remove_models:
            model_path = os.path.join(models_dir, model_file)
            
            # Ø­Ø°Ù ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
            related_files = [
                model_path,
                model_path.replace('.pt', '_info.json').replace('.pkl', '_info.json').replace('.joblib', '_info.json'),
                model_path.replace('.pt', '_scaler.pkl').replace('.pkl', '_scaler.pkl').replace('.joblib', '_scaler.pkl'),
                model_path.replace('.pt', '_label_encoder.pkl').replace('.pkl', '_label_encoder.pkl').replace('.joblib', '_label_encoder.pkl')
            ]
            
            for file_path in related_files:
                if os.path.exists(file_path):
                    try:
                        os.remove(file_path)
                        removed.append(os.path.basename(file_path))
                    except Exception as e:
                        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù {file_path}: {e}")
        
        print(f"ğŸ—‘ï¸  {len(removed)} ÙØ§ÛŒÙ„ Ø­Ø°Ù Ø´Ø¯Ù†Ø¯. {keep_best} Ù…Ø¯Ù„ Ø¨Ø±ØªØ± Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯")
        
        return removed
    
    @staticmethod
    def find_best_model(models_dir: str, model_type: str, 
                       metric: str = 'roc_auc_ovr') -> Tuple[Optional[str], Optional[Dict[str, Any]]]:
        """
        Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø± Ù…Ø´Ø®Øµ
        
        Parameters:
        -----------
        models_dir : str
            Ù¾ÙˆØ´Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        model_type : str
            Ù†ÙˆØ¹ Ù…Ø¯Ù„
        metric : str
            Ù…Ø¹ÛŒØ§Ø± Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†
        
        Returns:
        --------
        Tuple[Optional[str], Optional[Dict[str, Any]]]
            Ù…Ø³ÛŒØ± Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù†
        """
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ù†ÙˆØ¹ Ù…Ø´Ø®Øµ
        model_files = []
        for file in os.listdir(models_dir):
            if file.startswith(model_type) and (file.endswith('.pt') or file.endswith('.pkl') or file.endswith('.joblib')):
                model_files.append(file)
        
        if not model_files:
            print(f"âš ï¸ Ù‡ÛŒÚ† Ù…Ø¯Ù„ {model_type} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return None, None
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„
        best_model = None
        best_score = -np.inf
        best_info = None
        
        for model_file in model_files:
            model_path = os.path.join(models_dir, model_file)
            info_path = model_path.replace('.pt', '_info.json').replace('.pkl', '_info.json').replace('.joblib', '_info.json')
            
            if os.path.exists(info_path):
                with open(info_path, 'r', encoding='utf-8') as f:
                    info = json.load(f)
                
                metrics = info.get('metrics', {})
                score = metrics.get(metric, 0)
                
                if score > best_score:
                    best_score = score
                    best_model = model_path
                    best_info = info
        
        if best_model:
            print(f"ğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ {model_type}: {os.path.basename(best_model)}")
            print(f"   Ù…Ø¹ÛŒØ§Ø± {metric}: {best_score:.4f}")
        
        return best_model, best_info
    
    @staticmethod
    def compare_models(models_dir: str, metric: str = 'roc_auc_ovr') -> pd.DataFrame:
        """
        Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
        
        Parameters:
        -----------
        models_dir : str
            Ù¾ÙˆØ´Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        metric : str
            Ù…Ø¹ÛŒØ§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
        
        Returns:
        --------
        pd.DataFrame
            Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡
        """
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        model_data = []
        
        for file in os.listdir(models_dir):
            if file.endswith(('.pt', '.pkl', '.joblib')):
                model_path = os.path.join(models_dir, file)
                info_path = model_path.replace('.pt', '_info.json').replace('.pkl', '_info.json').replace('.joblib', '_info.json')
                
                if os.path.exists(info_path):
                    with open(info_path, 'r', encoding='utf-8') as f:
                        info = json.load(f)
                    
                    metrics = info.get('metrics', {})
                    
                    model_data.append({
                        'model_file': file,
                        'model_type': info.get('model_type', 'Unknown'),
                        'metric_score': metrics.get(metric, 0),
                        'save_time': info.get('save_time', ''),
                        'config': str(info.get('config', {}))
                    })
        
        # Ø§ÛŒØ¬Ø§Ø¯ DataFrame
        if model_data:
            df = pd.DataFrame(model_data)
            df = df.sort_values(by='metric_score', ascending=False)
            return df
        else:
            return pd.DataFrame()
    
    @staticmethod
    def export_model_for_production(model_path: str, output_dir: str) -> Dict[str, str]:
        """
        ØµØ§Ø¯Ø± Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯
        
        Parameters:
        -----------
        model_path : str
            Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒ
        output_dir : str
            Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
        
        Returns:
        --------
        Dict[str, str]
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØµØ§Ø¯Ø± Ø´Ø¯Ù‡
        """
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
        os.makedirs(output_dir, exist_ok=True)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
        model, info = ExamModelUtils.load_model(model_path)
        
        # Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
        base_name = os.path.basename(model_path).split('.')[0]
        exported_files = {}
        
        # Ú©Ù¾ÛŒ ÙØ§ÛŒÙ„ Ù…Ø¯Ù„
        if model_path.endswith('.pt'):
            exported_path = os.path.join(output_dir, f"{base_name}.pt")
            shutil.copy2(model_path, exported_path)
            exported_files['model'] = exported_path
        
        # Ú©Ù¾ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ
        related_files = [
            model_path.replace('.pt', '_info.json').replace('.pkl', '_info.json').replace('.joblib', '_info.json'),
            model_path.replace('.pt', '_scaler.pkl').replace('.pkl', '_scaler.pkl').replace('.joblib', '_scaler.pkl'),
            model_path.replace('.pt', '_label_encoder.pkl').replace('.pkl', '_label_encoder.pkl').replace('.joblib', '_label_encoder.pkl')
        ]
        
        for src_file in related_files:
            if os.path.exists(src_file):
                dst_file = os.path.join(output_dir, os.path.basename(src_file))
                shutil.copy2(src_file, dst_file)
                exported_files[os.path.basename(src_file).split('.')[0]] = dst_file
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ README
        readme_path = os.path.join(output_dir, 'README.md')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(f"# Ù…Ø¯Ù„ {info.get('model_type', 'Unknown')}\n\n")
            f.write(f"## Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„\n")
            f.write(f"- Ù†ÙˆØ¹: {info.get('model_type', 'Unknown')}\n")
            f.write(f"- ØªØ§Ø±ÛŒØ® Ø°Ø®ÛŒØ±Ù‡: {info.get('save_time', '')}\n")
            f.write(f"- Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§: {json.dumps(info.get('metrics', {}), indent=2, ensure_ascii=False)}\n")
            f.write(f"\n## Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡\n")
            f.write("```python\n")
            f.write("from exam_utils import ExamModelUtils\n")
            f.write("model, info = ExamModelUtils.load_model('model.pt')\n")
            f.write("```\n")
        
        exported_files['readme'] = readme_path
        
        print(f"âœ… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø± {output_dir} ØµØ§Ø¯Ø± Ø´Ø¯")
        
        return exported_files


def print_model_config(config: Dict[str, Any], title: str = "Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„") -> None:
    """
    Ú†Ø§Ù¾ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„ Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ¨Ø§
    
    Parameters:
    -----------
    config : Dict[str, Any]
        Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„
    title : str
        Ø¹Ù†ÙˆØ§Ù† Ù†Ù…Ø§ÛŒØ´
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ {title}")
    print(f"{'='*60}")
    
    for key, value in config.items():
        if isinstance(value, (list, tuple)):
            value_str = ', '.join(str(v) for v in value)
        elif isinstance(value, dict):
            value_str = json.dumps(value, ensure_ascii=False, indent=2)
        else:
            value_str = str(value)
        
        print(f"  {key}: {value_str}")
    
    print(f"{'='*60}")


def save_training_results(results: Dict[str, Any], save_path: str) -> None:
    """
    Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¢Ù…ÙˆØ²Ø´
    
    Parameters:
    -----------
    results : Dict[str, Any]
        Ù†ØªØ§ÛŒØ¬ Ø¢Ù…ÙˆØ²Ø´
    save_path : str
        Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡
    """
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ ÙØ±Ù…Øª JSON
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"âœ… Ù†ØªØ§ÛŒØ¬ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø± {save_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")


def load_training_results(load_path: str) -> Dict[str, Any]:
    """
    Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¢Ù…ÙˆØ²Ø´
    
    Parameters:
    -----------
    load_path : str
        Ù…Ø³ÛŒØ± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
    
    Returns:
    --------
    Dict[str, Any]
        Ù†ØªØ§ÛŒØ¬ Ø¢Ù…ÙˆØ²Ø´
    """
    if not os.path.exists(load_path):
        raise FileNotFoundError(f"ÙØ§ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ ÛŒØ§ÙØª Ù†Ø´Ø¯: {load_path}")
    
    with open(load_path, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    print(f"âœ… Ù†ØªØ§ÛŒØ¬ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² {load_path} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
    
    return results