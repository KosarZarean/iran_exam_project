"""
Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø§ØµÙ„ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
"""

import os
import sys
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import warnings
warnings.filterwarnings('ignore')

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø­ÛŒØ· Ø¨Ø±Ø§ÛŒ reproducibility
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
torch.backends.cudnn.benchmark = False

# ØªÙ†Ø¸ÛŒÙ… seedÙ‡Ø§ Ø¨Ø±Ø§ÛŒ reproducibility
torch.manual_seed(42)
np.random.seed(42)
torch.use_deterministic_algorithms(True)

# Import custom modules
from exam_data_manager import ExamDataManager, ExamDataAnalyzer
from exam_trainer import ExamModelTrainer
from exam_evaluator import ExamModelEvaluator
from exam_utils import ExamModelUtils


class IranExamPipeline:
    """
    Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†
    """

    def __init__(self, args):
        """
        Initialize pipeline with arguments

        Parameters:
        -----------
        args : dict or DotDict
            Pipeline arguments
        """
        self.args = args
        self.root_dir = Path(__file__).parent
        self.data_manager = None
        self.trainer = None
        self.evaluator = None

        # Setup directories
        self.setup_directories()

        # Setup logging
        self.setup_logging()

        print("ğŸ“ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù† Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
        print("="*60)

    def setup_directories(self):
        """Create necessary directories"""
        directories = [
            self.args.get('data_path', 'data'),
            self.args.get('models_path', 'models'),
            self.args.get('plots_path', 'plots'),
            self.args.get('results_path', 'results'),
            self.args.get('logs_path', 'logs')
        ]

        for directory in directories:
            dir_path = self.root_dir / directory
            dir_path.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ“ Ù¾ÙˆØ´Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {dir_path}")

    def setup_logging(self):
        """Setup logging and recording"""
        self.recording_file = self.root_dir / self.args.get('recording_file', 'pipeline_output.txt')

        # Clear previous recording if requested
        if self.args.get('clear_previous', True):
            with open(self.recording_file, 'w', encoding='utf-8') as f:
                f.write("ğŸ“ Ú¯Ø²Ø§Ø±Ø´ Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†\n")
                f.write("="*60 + "\n\n")

        print(f"ğŸ“ ÙØ§ÛŒÙ„ Ú¯Ø²Ø§Ø±Ø´: {self.recording_file}")

    def log_message(self, message):
        """Log message to file and console"""
        print(message)
        with open(self.recording_file, 'a', encoding='utf-8') as f:
            f.write(message + "\n")

    def run_data_pipeline(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        """
        self.log_message("\nğŸ“Š Ø´Ø±ÙˆØ¹ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")
        self.log_message("="*60)

        # 1. Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯ÛŒØ± Ø¯Ø§Ø¯Ù‡
        self.log_message("1. Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯ÛŒØ± Ø¯Ø§Ø¯Ù‡...")
        self.data_manager = ExamDataManager(
            data_dir=str(self.root_dir / self.args.get('data_path', 'data')),
            recording_file=str(self.recording_file),
            plots_folder=str(self.root_dir / self.args.get('plots_path', 'plots'))
        )

        # 2. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.log_message("2. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        df = self.data_manager.load_and_prepare_data(
            data_path=self.args.get('exam_data_path', 'data/iran_exam.csv'),
            task_type=self.args.get('task_type', 'classification')
        )

        # 3. ØªØ­Ù„ÛŒÙ„ Ø§Ú©ØªØ´Ø§ÙÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.log_message("3. ØªØ­Ù„ÛŒÙ„ Ø§Ú©ØªØ´Ø§ÙÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        self.data_manager.exploratory_data_analysis()

        # 4. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        self.log_message("4. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§...")
        if 'TabTransformer' in self.args.get('models', []):
            self.data_manager.prepare_for_tabtransformer()

        self.data_manager.prepare_for_traditional_models()

        # 5. Ø§ÛŒØ¬Ø§Ø¯ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.log_message("5. Ø§ÛŒØ¬Ø§Ø¯ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        splits = self.data_manager.create_train_val_test_split(
            train_size=self.args.get('train_size', 0.7),
            val_size=self.args.get('val_size', 0.15),
            test_size=self.args.get('test_size', 0.15)
        )

        self.log_message("âœ… Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯")
        return df

    def run_training_pipeline(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        """
        self.log_message("\nğŸ¯ Ø´Ø±ÙˆØ¹ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§")
        self.log_message("="*60)

        if self.data_manager is None:
            self.log_message("âŒ Ø§Ø¨ØªØ¯Ø§ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯")
            return None

        # 1. Ø§ÛŒØ¬Ø§Ø¯ Trainer
        self.log_message("1. Ø§ÛŒØ¬Ø§Ø¯ Trainer...")
        self.trainer = ExamModelTrainer(
            data_manager=self.data_manager,
            output_dir=str(self.root_dir / self.args.get('models_path', 'models')),
            random_state=self.args.get('random_state', 42)
        )

        # 2. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ
        models_to_train = self.args.get('models', ['RandomForest', 'MLP', 'TabTransformer'])
        self.log_message(f"2. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§: {models_to_train}")

        all_results = {}
        for model_type in models_to_train:
            try:
                self.log_message(f"  ğŸ¯ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„: {model_type}")

                results = self.trainer.nested_cross_validation(
                    model_type=model_type,
                    k_outer=self.args.get('k_fold_cv', 5),
                    k_inner=self.args.get('k_inner_cv', 3)
                )

                all_results[model_type] = results
                self.log_message(f"  âœ… Ø¢Ù…ÙˆØ²Ø´ {model_type} Ú©Ø§Ù…Ù„ Ø´Ø¯")

            except Exception as e:
                self.log_message(f"  âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ {model_type}: {e}")

        # 3. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        self.log_message("3. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§...")
        comparison = self.trainer.compare_models(models_to_train)

        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù…Ù‚Ø§ÛŒØ³Ù‡
        comparison_path = self.root_dir / self.args.get('results_path', 'results') / 'model_comparison.csv'
        comparison.to_csv(comparison_path, index=False, encoding='utf-8-sig')
        self.log_message(f"  ğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ø± {comparison_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

        self.log_message("âœ… Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯")
        return all_results

    def run_evaluation_pipeline(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        """
        self.log_message("\nğŸ§ª Ø´Ø±ÙˆØ¹ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§")
        self.log_message("="*60)

        if self.data_manager is None:
            self.log_message("âŒ Ø§Ø¨ØªØ¯Ø§ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯")
            return None

        # 1. Ø§ÛŒØ¬Ø§Ø¯ Evaluator
        self.log_message("1. Ø§ÛŒØ¬Ø§Ø¯ Evaluator...")
        self.evaluator = ExamModelEvaluator(
            data_manager=self.data_manager,
            output_dir=str(self.root_dir / self.args.get('results_path', 'results'))
        )

        # 2. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
        self.log_message("2. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡...")
        evaluation_results = self.evaluator.evaluate_all_models(
            models_dir=str(self.root_dir / self.args.get('models_path', 'models')),
            output_dir=str(self.root_dir / self.args.get('results_path', 'results'))
        )

        self.log_message("âœ… Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯")
        return evaluation_results

    def run_complete_pipeline(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø§Ø² Ø§Ø¨ØªØ¯Ø§ ØªØ§ Ø§Ù†ØªÙ‡Ø§
        """
        self.log_message("\nğŸš€ Ø´Ø±ÙˆØ¹ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ†")
        self.log_message("="*60)

        start_time = pd.Timestamp.now()
        self.log_message(f"â° Ø²Ù…Ø§Ù† Ø´Ø±ÙˆØ¹: {start_time}")

        # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø±Ø§Ø­Ù„
        try:
            # 1. Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            df = self.run_data_pipeline()

            # 2. Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¢Ù…ÙˆØ²Ø´
            training_results = self.run_training_pipeline()

            # 3. Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
            evaluation_results = self.run_evaluation_pipeline()

            end_time = pd.Timestamp.now()
            duration = end_time - start_time

            self.log_message(f"\nâœ… Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯!")
            self.log_message(f"â±ï¸  Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {duration}")

            return {
                'data': df,
                'training': training_results,
                'evaluation': evaluation_results,
                'duration': duration
            }

        except Exception as e:
            self.log_message(f"\nâŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ†: {e}")
            import traceback
            traceback.print_exc()
            return None


class DotDict(dict):
    """Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ø¯Ø³ØªØ±Ø³ÛŒ dot notation"""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__


def get_exam_args():
    """
    ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ú©Ù†Ú©ÙˆØ±
    """
    args = {
        # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡
        'exam_data_path': 'data/iran_exam.csv',
        'data_path': 'exam_data',
        'models_path': 'exam_models',
        'plots_path': 'exam_plots',
        'results_path': 'exam_results',
        'logs_path': 'exam_logs',

        # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
        'recording_file': 'exam_pipeline_output.txt',

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ†
        'clear_previous': True,
        'random_state': 42,
        'task_type': 'classification',  # ÛŒØ§ 'regression'

        # ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        'train_size': 0.7,
        'val_size': 0.15,
        'test_size': 0.15,

        # Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
        'models': ['RandomForest', 'XGBoost', 'LightGBM', 'MLP', 'TabTransformer'],

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Cross-Validation
        'k_fold_cv': 5,
        'k_inner_cv': 3,

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø¶Ø§ÙÛŒ
        'use_gpu': True,
        'verbose': True,
        'save_all_models': True
    }

    return DotDict(args)


def main():
    """
    ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ†
    """
    print("ğŸ“ Ù¾Ø±ÙˆÚ˜Ù‡ Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ú©ÙˆØ± Ø§ÛŒØ±Ø§Ù†")
    print("="*60)

    # Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§
    args = get_exam_args()

    # Ø§ÛŒØ¬Ø§Ø¯ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ†
    pipeline = IranExamPipeline(args)

    # Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ†
    results = pipeline.run_complete_pipeline()

    if results:
        print("\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬:")
        if results.get('training'):
            for model_type, model_results in results['training'].items():
                if 'mean_test_score' in model_results:
                    print(f"   {model_type}: {model_results['mean_test_score']:.4f}")

    print("\nâœ… Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!")
    print("="*60)


if __name__ == '__main__':
    main()